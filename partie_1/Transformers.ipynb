{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données d'entrainement\n",
    "train_data_complete = pd.read_csv(\"../data/allocine_genres_train.csv\", sep=\",\")\n",
    "train_data = train_data_complete[[\"titre\", \"synopsis\", \"genre\"]]\n",
    "\n",
    "# Données de test/validation\n",
    "test_data_complete = pd.read_csv(\"../data/allocine_genres_test.csv\", sep=\",\")\n",
    "test_data = test_data_complete[[\"titre\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de classes et ajout d'un identifiant numérique pour chaque classe\n",
    "genre_name = sorted(train_data.genre.unique().flatten())\n",
    "print(\"Genres:\", genre_name)\n",
    "print(\"Nombre d'exemplaires:\", len(train_data))\n",
    "\n",
    "label2id = {genre_name[i]:i for i in range(len(genre_name))}\n",
    "id2label = {i:genre_name[i] for i in range(len(genre_name))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "batch_size = 64\n",
    "# Proportion des données qui sera utilisée\n",
    "scale = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value, ClassLabel, Dataset, DatasetDict\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "# Chaque film est décrit par un titre et un synopsis\n",
    "split_titre = train_data.titre\n",
    "data_df[\"text\"] = split_titre + \" \" + train_data.synopsis\n",
    "# Chaque filme possède un attribut qui décrit son genre (auquel a été associé un identifiant numérique)\n",
    "data_df[\"genre\"] = train_data.genre.map(label2id)\n",
    "\n",
    "# Transformation du DataFrame en objet de type Dataset utilisé par HuggingFace\n",
    "province_features = Features({\"text\": Value('string'),\n",
    "                              \"genre\": ClassLabel(names=genre_name)})\n",
    "data = Dataset.from_pandas(data_df, features=province_features)\n",
    "# Découpage en train et test\n",
    "data = data.train_test_split(test_size=0.2, shuffle=True, seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokénisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_ckpt = \"camembert-base\"\n",
    "# model_ckpt = \"google/flan-t5-large\"\n",
    "# model_ckpt = \"xlm-roberta-base\" # Trop gros\n",
    "# model_ckpt = \"t5-small\"\n",
    "model_ckpt = \"distilbert-base-multilingual-cased\"\n",
    "\n",
    "# Chargement du tokéniseur pré-entraîné correspondant au modèle utilisé\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokénisation des 2 premières instances\n",
    "preprocess_function(data[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation de la totalité des données : chaque unité est remplacée par un identifiant numérique\n",
    "tokenized_data = data.map(preprocess_function, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des tokens. DistilBERT utilise l'algorithme WordPiece\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_data[\"train\"][0]['input_ids'])\n",
    "print(tokenized_data[\"train\"][0][\"text\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille du vocabulaire\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille de contexte maximum\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de l'Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction et Entraînement du Transformer (PyTorch -> NE FONCTIONNE PAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_ckpt, num_labels=len(genre_name), id2label=id2label, label2id=label2id\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=f\"{model_ckpt}-finetuned-genre\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=6,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_data[\"train\"],\n",
    "#     eval_dataset=tokenized_data[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction et Entraînement du Transformer (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_data[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=9, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_data[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_data[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "# metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set, label_cols=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"my_awesome_model\",\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [metric_callback, push_to_hub_callback]\n",
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks, target=)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions pour les données de test\n",
    "preds_output = trainer.predict(tokenized_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = tokenized_data['test']['label']\n",
    "labels = tokenized_data['test'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    labels_for_fig = [l[0:4]+'.' for l in labels]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                  display_labels=labels_for_fig)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Fonction qui retourne la perte (entropie croisée) et la classe prédite\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n",
    "                             reduction=\"none\")\n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des données au bon format\n",
    "tokenized_data.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des valeurs de perte\n",
    "tokenized_data[\"test\"] = tokenized_data[\"test\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame avec les textes, les pertes les classe (prédites et attendues)\n",
    "\n",
    "def label_int2str(row):\n",
    "    return tokenized_data[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "tokenized_data.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = tokenized_data[\"test\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                              .apply(label_int2str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour éviter l'affichage tronqué des descriptions\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "# Affichage des 10 premières instances triées par perte décroissante\n",
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des 10 premières instances triées par perte croissante\n",
    "# Cela permet de voir les instances pour lesquelles les prédictions sont les plus certaines\n",
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
