{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données d'entrainement\n",
    "train_data_complete = pd.read_csv(\"../data/allocine_genres_train.csv\", sep=\",\")\n",
    "train_data = train_data_complete[[\"titre\", \"synopsis\", \"genre\"]]\n",
    "\n",
    "# Données de test/validation\n",
    "test_data_complete = pd.read_csv(\"../data/allocine_genres_test.csv\", sep=\",\")\n",
    "test_data = test_data_complete[[\"titre\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lister les classes et leur associer un identifiant unique. (Utile pour le plongement des mots et pour l'entraînement du CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des genres\n",
    "genre_name = sorted(train_data.genre.unique().flatten())\n",
    "print(\"Genres:\", genre_name)\n",
    "print(\"Nombre d'exemplaires:\", len(train_data))\n",
    "\n",
    "# Identifiant unique par genre\n",
    "genre_index = {genre_name[i]:i for i in range(len(genre_name))}\n",
    "genre_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplacer les genres par la valeur numérique associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace({\"genre\": genre_index})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data[[\"titre\", \"synopsis\"]],\n",
    "                                                    train_data[[\"genre\"]],\n",
    "                                                    test_size=0.001,\n",
    "                                                    random_state=12, # Random seed for shuffle\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On combine le titre et le synopsis pour pouvoir les vectoriser par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_titre = X_train.titre\n",
    "X_train = X_train_titre + \" \" + X_train.synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation du Vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(documents, max_voc_size=8000, max_seq_length= 200, batch_size=64):\n",
    "\tvectorizer = tf.keras.layers.TextVectorization(max_tokens=max_voc_size, output_sequence_length=max_seq_length)\n",
    "\t# Création du jeu de données à partir de X_train et constitution de lots de 128 instances\n",
    "\ttext_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n",
    "\t# Création du vocabulaire à partir des données d'entrée\n",
    "\tvectorizer.adapt(text_ds)\n",
    "\treturn vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = get_vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texte initial:\", X_train.iloc[1])\n",
    "output = vectorizer([X_train.iloc[1]])\n",
    "print(\"Vocabulaire dans le texte (15 premiers items):\")\n",
    "for v in output.numpy()[0, :15]:\n",
    "    print(v, vectorizer.get_vocabulary()[v])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de Plongements de Mots Pré-entraînés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"../embedding/frWiki_no_phrase_no_postag_700_cbow_cut100.bin\", binary=True, unicode_errors=\"ignore\")\n",
    "model.most_similar(\"bonjour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"bonjour\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le plongement pré-entrainé est de dimension 700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model[\"bonjour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_model):\n",
    "    embeddings_index = {}\n",
    "    for word in list(embeddings_model.key_to_index.keys()):\n",
    "        embeddings_index[word] = embeddings_model[word]\n",
    "    print(f'{len(embeddings_index)} vecteurs de mots ont été lus')\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante permet de créer une matrice de plongements: une matrice où la ligne i correspond au plongement pré-entraîné pour le mot d'indice i dans le vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocabulary, embeddings_index, embedding_dim = 700):\n",
    "  num_tokens = len(vocabulary)\n",
    "  hits = 0\n",
    "  misses = 0\n",
    "\n",
    "  # Préparation de la matrice\n",
    "  # Les mots qui ne se trouvent pas dans les plongements pré-entraînés seront \n",
    "  # représentés par des vecteurs dont toutes les composantes sont égales à 0,\n",
    "  # y compris la représentation utilisée pour compléter les documents courts et\n",
    "  # celle utilisée pour les mots inconnus [UNK]\n",
    "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "  for word, i in word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "          hits += 1\n",
    "      else:\n",
    "          misses += 1\n",
    "  print(f'{hits} mots ont été trouvés dans les plongements pré-entraînés')\n",
    "  print(f'{misses} sont absents')\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de la matrice de plongements à partir du vocabulaire\n",
    "embedding_matrix = get_embedding_matrix(voc, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def get_biLSTM_model(voc_size, embedding_matrix, embedding_dim=700):\n",
    "  # Création du modèle\n",
    "  int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "  embedding_layer = Embedding(voc_size, embedding_dim, trainable=True,\n",
    "      embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "  )\n",
    "  \n",
    "  embedded_sequences = embedding_layer(int_sequences_input)\n",
    "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(embedded_sequences)\n",
    "  preds = tf.keras.layers.Dense(len(genre_name), activation=\"softmax\")(x)\n",
    "  model = tf.keras.Model(int_sequences_input, preds)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de l'architecture du modèle\n",
    "biLSTM_model = get_biLSTM_model(len(voc), embedding_matrix)\n",
    "biLSTM_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from tensorflow.keras.callbacks import EarlyStopping # Early stopping to avoid over-fitting\n",
    "\n",
    "# Fonction pour l'entraînement d'un modèle\n",
    "def train_model(X, y, model_function, vectorizer, voc_size, embedding_matrix, embedding_dim=700, batch_size=32): # 128\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_acc\", restore_best_weights=True, patience=3)\n",
    "    \n",
    "    # Listes utilisées pour sauvegarder les résultats obtenus à chaque pli\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    histories = []\n",
    "    folds = 3 # 5\n",
    "    stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, random_state=12)\n",
    "    fold_no = 1\n",
    "    for train, test in stratkfold.split(X, y):\n",
    "        m_function = globals()[model_function]\n",
    "        model = m_function(voc_size, embedding_matrix, embedding_dim)\n",
    "\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Entraînement pour le pli {fold_no} ...')\n",
    "        fold_x_train = vectorizer(X.iloc[train].to_numpy()).numpy()\n",
    "        fold_x_val = vectorizer(X.iloc[test].to_numpy()).numpy()\n",
    "        fold_y_train = y.iloc[train].to_numpy()\n",
    "        fold_y_val = y.iloc[test].to_numpy()\n",
    "\n",
    "        # Compilation du modèle : permet de préciser la fonction de perte et l'optimiseur\n",
    "        # loss=sparse_categorical_crossentropy : entropie croisée, dans le cas où les \n",
    "        # classes cibles sont indiquées sous forme d'entiers. Il s'agira de minimiser\n",
    "        # la perte pendant l'apprentissage\n",
    "        # optimizer=rmsprop : l'optimiseur détermine la manière doit les poids seront\n",
    "        # mis à jour pendant l'apprentissage\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "        # Entraînement sur 10 époque (la totalité du jeu de données est parcourue\n",
    "        # 10 fois)\n",
    "        history = model.fit(fold_x_train, fold_y_train, batch_size=batch_size, epochs=25, validation_data=(fold_x_val, fold_y_val), callbacks=[early_stopping_callback])\n",
    "        histories.append(history)\n",
    "        # Evaluation sur les données de validation\n",
    "        scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n",
    "        print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n",
    "            f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    # Affichage des scores moyens par pli\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print('Scores par pli')\n",
    "    for i in range(0, len(acc_per_fold)):\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}', f'- Accuracy: {acc_per_fold[i]:.2f}%')\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print('Scores moyens pour tous les plis :')\n",
    "    print(f'> Accuracy: {np.mean(acc_per_fold):.2f}', f'(+- {np.std(acc_per_fold):.2f})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
    "    print('---------------------------------------------------------------------')\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle et récupération des résultats\n",
    "biLSTM_histories = train_model(X_train, y_train, 'get_biLSTM_model', vectorizer, len(voc), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "def plot_results(histories):\n",
    "    accuracy_data = []\n",
    "    loss_data = []\n",
    "    for i, h in enumerate(histories):\n",
    "        acc = h.history['acc']\n",
    "        val_acc = h.history['val_acc']\n",
    "        loss = h.history['loss']\n",
    "        val_loss = h.history['val_loss']\n",
    "        for j in range(len(acc)):\n",
    "            accuracy_data.append([i+1, j+1, acc[j], 'Entraînement'])\n",
    "            accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n",
    "            loss_data.append([i+1, j+1, loss[j], 'Entraînement'])\n",
    "            loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n",
    "\n",
    "    acc_df = pd.DataFrame(accuracy_data, columns=['Pli', 'Epoch', 'Accuracy', 'Données'])\n",
    "    sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Données', kind='line')\n",
    "    \n",
    "    loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Données'])\n",
    "    sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Données', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(CNN_histories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
