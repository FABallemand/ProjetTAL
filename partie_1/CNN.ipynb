{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1: Classification de Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 19:08:26.593658: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 19:08:27.016426: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-24 19:08:27.025255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-24 19:08:27.025281: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-24 19:08:28.823346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 19:08:28.823510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 19:08:28.823526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/fabien/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données d'entrainement\n",
    "train_data_complete = pd.read_csv(\"../data/allocine_genres_train.csv\", sep=\",\")\n",
    "train_data = train_data_complete[[\"titre\", \"synopsis\", \"genre\"]]\n",
    "\n",
    "# Données de test/validation\n",
    "test_data_complete = pd.read_csv(\"../data/allocine_genres_test.csv\", sep=\",\")\n",
    "test_data = test_data_complete[[\"titre\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lister les classes et leur associer un identifiant unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres: ['biopic', 'comédie', 'documentaire', 'drame', 'historique', 'horreur', 'policier', 'romance', 'science fiction']\n",
      "Nombre d'exemplaires: 2875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'biopic': 0,\n",
       " 'comédie': 1,\n",
       " 'documentaire': 2,\n",
       " 'drame': 3,\n",
       " 'historique': 4,\n",
       " 'horreur': 5,\n",
       " 'policier': 6,\n",
       " 'romance': 7,\n",
       " 'science fiction': 8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liste des genres\n",
    "genre_name = sorted(train_data.genre.unique().flatten())\n",
    "print(\"Genres:\", genre_name)\n",
    "print(\"Nombre d'exemplaires:\", len(train_data))\n",
    "\n",
    "# Identifiant unique par genre\n",
    "genre_index = {genre_name[i]:i for i in range(len(genre_name))}\n",
    "genre_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplacer les genres par la valeur numérique associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le Crime de l' Orient - Express</td>\n",
       "      <td>En visite à Istanbul , le célèbre détective be...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 hommes en colère</td>\n",
       "      <td>Un jeune homme d' origine modeste est accusé d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Après moi le bonheur</td>\n",
       "      <td>Lorsque Marie-Laure , mère de quatre jeunes en...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Les Lumières de la ville</td>\n",
       "      <td>Un vagabond s’ éprend d’ une belle et jeune ve...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les Chemins de la dignité</td>\n",
       "      <td>L' histoire vraie de Carl Brashear , premier A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             titre  \\\n",
       "0  Le Crime de l' Orient - Express   \n",
       "1              12 hommes en colère   \n",
       "2             Après moi le bonheur   \n",
       "3         Les Lumières de la ville   \n",
       "4        Les Chemins de la dignité   \n",
       "\n",
       "                                            synopsis  genre  \n",
       "0  En visite à Istanbul , le célèbre détective be...      6  \n",
       "1  Un jeune homme d' origine modeste est accusé d...      3  \n",
       "2  Lorsque Marie-Laure , mère de quatre jeunes en...      3  \n",
       "3  Un vagabond s’ éprend d’ une belle et jeune ve...      7  \n",
       "4  L' histoire vraie de Carl Brashear , premier A...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.replace({\"genre\": genre_index})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data[[\"titre\", \"synopsis\"]],\n",
    "                                                    train_data[[\"genre\"]],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=12, # Random seed for shuffle\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On combine le titre et le synopsispour pouvoir les vectoriser par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_titre = X_train.titre\n",
    "X_train = X_train_titre + \" \" + X_train.synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2447    Le Coeur des hommes Alex , Antoine , Jeff et M...\n",
       "2741    Sliver Dirigeante d' une maison d' édition , C...\n",
       "2051    Grande-Synthe Crise migratoire , pollution ind...\n",
       "474     Dallas Buyers Club 1986 , Dallas , Texas , une...\n",
       "1037    Resident Evil : Chapitre Final Alice , seule s...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      genre\n",
       "2447      7\n",
       "2741      6\n",
       "2051      2\n",
       "474       0\n",
       "1037      5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2300,)\n",
      "(2300, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation du Vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(documents, max_voc_size=8000, max_seq_length= 200, batch_size=64):\n",
    "\tvectorizer = tf.keras.layers.TextVectorization(max_tokens=max_voc_size, output_sequence_length=max_seq_length)\n",
    "\t# Création du jeu de données à partir de X_train et constitution de lots de 128 instances\n",
    "\ttext_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n",
    "\t# Création du vocabulaire à partir des données d'entrée\n",
    "\tvectorizer.adapt(text_ds)\n",
    "\treturn vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 19:08:34.534061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-24 19:08:34.534130: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-24 19:08:34.534169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fabien): /proc/driver/nvidia/version does not exist\n",
      "2023-04-24 19:08:34.534649: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "keras_vectorizer = get_vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "voc = keras_vectorizer.get_vocabulary()\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'de', 'la', 'et', 'le', 'à', 'un', 'une', 'les']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte initial: Sliver Dirigeante d' une maison d' édition , Carly Norris s' installe dans un building ultra moderne de New York : le « Sliver » . Son emménagement se déroule sans encombre si ce n' est le désagréable accueil que lui réserve un voisin . Un frisson parcourt la nuque de Carly lorsqu' il lui annonce qu' elle ressemble étonnamment à la précédente locataire , qui s' est suicidée . Intriguée , Carly espère en savoir un peu plus mais Gus décède lui aussi . C' est alors qu' elle fait la connaissance de ses autres voisins : Zeke et Jack . Elle constate avec amusement que ces deux hommes ne s' apprécient guère . Un malaise s' installe néanmoins . Carly se sent constamment épiée . Sharon Stone , au sommet de sa réputation , illumine ce thriller primé aux MTV Awards .\n",
      "Vocabulaire dans le texte (15 premiers items):\n",
      "7771 sliver\n",
      "1 [UNK]\n",
      "16 d\n",
      "8 une\n",
      "140 maison\n",
      "16 d\n",
      "3538 édition\n",
      "3440 carly\n",
      "1 [UNK]\n",
      "37 s\n",
      "608 installe\n",
      "17 dans\n",
      "7 un\n",
      "1 [UNK]\n",
      "2659 ultra\n"
     ]
    }
   ],
   "source": [
    "print(\"Texte initial:\", X_train.iloc[1])\n",
    "output = keras_vectorizer([X_train.iloc[1]])\n",
    "print(\"Vocabulaire dans le texte (15 premiers items):\")\n",
    "for v in output.numpy()[0, :15]:\n",
    "    print(v, keras_vectorizer.get_vocabulary()[v])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement de Plongements de Mots Pré-entraînés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('merci', 0.7507892847061157),\n",
       " ('bonsoir', 0.7450243830680847),\n",
       " ('votre', 0.5642200112342834),\n",
       " ('vous', 0.5538792014122009),\n",
       " ('remercier', 0.5396129488945007),\n",
       " ('avance', 0.5288880467414856),\n",
       " ('discuter', 0.5033395886421204),\n",
       " ('je', 0.49339333176612854),\n",
       " ('désoler', 0.4899965822696686),\n",
       " ('ici', 0.4887441396713257)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"../embedding/frWiki_no_phrase_no_postag_700_cbow_cut100.bin\", binary=True, unicode_errors=\"ignore\")\n",
    "model.most_similar(\"bonjour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_file):\n",
    "    embeddings_index = {}\n",
    "    with open(embeddings_file, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f'{len(embeddings_index)} vecteurs de mots ont été lus')\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xab in position 15: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/fabien/TPS/2A/Algorithmes du Texte/ProjetTAL/partie_1/CNN.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/fabien/TPS/2A/Algorithmes%20du%20Texte/ProjetTAL/partie_1/CNN.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m6_embeddings \u001b[39m=\u001b[39m load_embeddings(\u001b[39m\"\u001b[39;49m\u001b[39m../embedding/frWiki_no_phrase_no_postag_700_cbow_cut100.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/fabien/TPS/2A/Algorithmes du Texte/ProjetTAL/partie_1/CNN.ipynb Cell 26\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(embeddings_file)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fabien/TPS/2A/Algorithmes%20du%20Texte/ProjetTAL/partie_1/CNN.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings_index \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fabien/TPS/2A/Algorithmes%20du%20Texte/ProjetTAL/partie_1/CNN.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(embeddings_file, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/fabien/TPS/2A/Algorithmes%20du%20Texte/ProjetTAL/partie_1/CNN.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fabien/TPS/2A/Algorithmes%20du%20Texte/ProjetTAL/partie_1/CNN.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         word, coefs \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(maxsplit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fabien/TPS/2A/Algorithmes%20du%20Texte/ProjetTAL/partie_1/CNN.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         coefs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfromstring(coefs, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xab in position 15: invalid start byte"
     ]
    }
   ],
   "source": [
    "m6_embeddings = load_embeddings(\"../embedding/frWiki_no_phrase_no_postag_700_cbow_cut100.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
