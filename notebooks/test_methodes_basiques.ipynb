{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import spacy\n",
    "\n",
    "# better display of review text in dataframes\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "# Seaborn options\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download fr_core_news_sm\n",
    "spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download(\"stopwords\")\n",
    "stopWords = set(stopwords.words(\"french\"))\n",
    "print(stopWords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# PICKLE_PATH = \"allocine_dataset/data/allocine_dataset.pickle\"\n",
    "\n",
    "# with open(PICKLE_PATH, 'rb') as reader:\n",
    "#     data = pickle.load(reader)\n",
    "\n",
    "# X_train, y_train = np.array(data[\"train_set\"]['review']), np.array(data[\"train_set\"]['polarity'])\n",
    "# X_val, y_val = np.array(data[\"val_set\"]['review']), np.array(data[\"val_set\"]['polarity'])\n",
    "# X_test, y_test = np.array(data[\"test_set\"]['review']), np.array(data[\"test_set\"]['polarity'])\n",
    "# class_names = data['class_names']\n",
    "\n",
    "# print(\"LEN TRAIN: \"+ str(len(X_train)))\n",
    "# print(\"LEN VAL: \"+ str(len(X_val)))\n",
    "# print(\"LEN TEST: \"+ str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess(sentence) :\n",
    "    list_w = nlp(sentence)\n",
    "    list_w_clean = []\n",
    "    res = []\n",
    "    for token in list_w:\n",
    "        if (token.text.lower() not in stopWords) and (token.text not in string.punctuation):\n",
    "            list_w_clean.append(token)\n",
    "    for token in list_w_clean:\n",
    "        res.append(token.lemma_.lower())\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es d'entrainement\n",
    "train_data_complete = pd.read_csv(\"../data/allocine_genres_train.csv\", sep=\",\")\n",
    "train_data = train_data_complete[[\"titre\", \"synopsis\", \"genre\"]]\n",
    "\n",
    "X = train_data.drop('genre', axis=1)\n",
    "y = train_data['genre']\n",
    "\n",
    "# Initialize the RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "#ros = RandomUnderSampler()\n",
    "\n",
    "X[\"titre\"] = X[\"titre\"].apply(preprocess)\n",
    "X[\"synopsis\"] = X[\"synopsis\"].apply(preprocess)\n",
    "\n",
    "# Perform oversampling\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Convert the resampled data back to a DataFrame\n",
    "X_train = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "X_train = X_train[\"synopsis\"]\n",
    "y_train = pd.DataFrame(y_resampled)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=12, # Random seed for shuffle\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data['genre']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tfidf_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()), # Default parameters\n",
    "    ('clf', LogisticRegression(n_jobs=-1, verbose=1)),\n",
    "])\n",
    "\n",
    "tfidf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 0\n",
    "some_review = X_val[some_index]\n",
    "print(some_review)\n",
    "print()\n",
    "print(\"True Polarity:\", class_names[y_val[some_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_prediction = tfidf_clf.predict([some_review, ])\n",
    "print(\"Predicted Polarity:\", class_names[some_prediction[0]]) # Good prediction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Predicting training dataset\n",
    "y_pred = tfidf_clf.predict(X_train)\n",
    "print(\"Training Accuracy:\", metrics.accuracy_score(y_train, y_pred))\n",
    "\n",
    "# Predicting with a test dataset\n",
    "\n",
    "y_pred = tfidf_clf.predict(X_val)\n",
    "print(\"Validation Accuracy:\", metrics.accuracy_score(y_val, y_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_val, y_pred, target_names=class_names.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# Grid-search on validation set.\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "y = np.concatenate((y_train, y_val), axis=0)\n",
    "validation_indexes = [-1]*len(X_train) + [0]*len(X_val)\n",
    "ps = PredefinedSplit(test_fold=validation_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "     'tfidf__lowercase': (True, False),\n",
    "     'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__max_df': [0.60, 0.65, 0.70, 0.75, 0.85, 1],\n",
    "     'clf__C': np.logspace(-4, 4, 10),\n",
    "}\n",
    "\n",
    "tfidf_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', n_jobs=-1, verbose=1)),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    tfidf_clf, param_grid, cv=ps, \n",
    "    scoring='accuracy', return_train_score=True, \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_) # pprint ?\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "best_clf = grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to grid search,  validation accuracy is now ~2 percents higher than before !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        lowercase=True, ngram_range=(1, 2),\n",
    "        max_df=0.75\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        C=1300, penalty='l2', \n",
    "        n_jobs=-1, verbose=1\n",
    "    )),\n",
    "])\n",
    "\n",
    "best_clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "y_pred = best_clf.predict(X_val)\n",
    "\n",
    "print(\"Val Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y_val, y_pred)))\n",
    "print(\"Val F1-Score: {:.2f}\".format(100 * metrics.f1_score(y_val, y_pred)))\n",
    "print()\n",
    "\n",
    "report = metrics.classification_report(\n",
    "    y_val, y_pred, \n",
    "    target_names=class_names.values()\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Create CV training and test scores for various training set sizes\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_clf, X, y, cv=ps, \n",
    "    scoring='accuracy',n_jobs=-1,verbose=1,\n",
    "    # 50 different sizes of the training set\n",
    "    train_sizes=np.linspace(0.01, 1.0, 50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def plot_learning_curves(train_sizes, train_scores, test_scores, figsize=(10,7), fontsize=14):\n",
    "    # Create means and standard deviations of training set scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    \n",
    "    # Create means and standard deviations of test set scores\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Draw lines\n",
    "    plt.plot(train_sizes, train_mean, '--', color=\"r\",  label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_mean, color=\"g\", label=\"Validation score\")\n",
    "\n",
    "    # Draw bands\n",
    "    #plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "    #plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\") \n",
    "\n",
    "    # Create plot\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc=\"lower right\")  \n",
    "    \n",
    "       \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_learning_curves(train_sizes, train_scores, test_scores)\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "# Saving plot\n",
    "fig.savefig('img/tf-idf/learning_curves.png', dpi=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly that the training score is still around the maximum and the validation score could be increased with more training samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "fig = print_confusion_matrix(\n",
    "    conf_mx, \n",
    "    class_names.values(), \n",
    "    figsize=(7,5)\n",
    ")\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "# Saving plot\n",
    "fig.savefig('img/tf-idf/val_confusion_mx.png', dpi=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive / Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos = X_val[(y_val == 0) & (y_pred == 1)]\n",
    "false_neg = X_val[(y_val == 1) & (y_pred == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(false_pos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(false_neg[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/tf-idf/best_clf.pickle', 'wb') as f:\n",
    "    pickle.dump(best_clf, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/tf-idf/best_clf.pickle', 'rb') as f:\n",
    "    best_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"Test F1-Score: {:.2f}\".format(100 * metrics.f1_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "report = metrics.classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=class_names.values()\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = print_confusion_matrix(\n",
    "    conf_mx, \n",
    "    class_names.values(), \n",
    "    figsize=(7,5)\n",
    ")\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "# Saving plot\n",
    "fig.savefig('img/tf-idf/test_confusion_mx.png', dpi=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "sizes = [1000, 5000, 10000, 20000, 40000, 80000, 120000, 160000]\n",
    "\n",
    "def accuracy_vs_train_size(model, X_train, y_train, X_test, y_test, sizes):    \n",
    "    test_accuracies = []\n",
    "    for size in sizes:\n",
    "        # Train model on data subset\n",
    "        model.fit(X_train[:size], y_train[:size])\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    return test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = accuracy_vs_train_size(\n",
    "    best_clf, X_train, y_train,\n",
    "    X_test, y_test, sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "OUTPUT_PATH = 'data/tf-idf/tfidf_accuracies.pickle'\n",
    "\n",
    "output_dict = {\n",
    "    \"sizes\": sizes,\n",
    "    \"test_accuracies\": test_accuracies\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as writer:\n",
    "    pickle.dump(output_dict, writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/tf-idf/best_clf.pickle', 'rb') as f:\n",
    "    best_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "inference_times = []\n",
    "\n",
    "for i in range(1000):\n",
    "    x = np.array([X_test[i], ])\n",
    "    start_time = time.time()\n",
    "    y_pred = best_clf.predict(x)\n",
    "    stop_time = time.time()\n",
    "    \n",
    "    inference_times.append(stop_time - start_time)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/tf-idf/tfidf_times.pickle'\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as writer:\n",
    "    pickle.dump(inference_times, writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/tf-idf/best_clf.pickle', 'rb') as f:\n",
    "    best_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils_acl import get_data\n",
    "\n",
    "ACL_FOLDER = 'data/cls-acl10-unprocessed/fr'\n",
    "BOOKS_FOLDER = os.path.join(ACL_FOLDER, 'books')\n",
    "DVD_FOLDER = os.path.join(ACL_FOLDER, 'dvd')\n",
    "MUSIC_FOLDER = os.path.join(ACL_FOLDER, 'music')\n",
    "\n",
    "_, _, X_test_b, y_test_b = get_data(BOOKS_FOLDER)\n",
    "_, _, X_test_d, y_test_d = get_data(DVD_FOLDER)\n",
    "_, _, X_test_m, y_test_m = get_data(MUSIC_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    print(\"Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y, y_pred)))\n",
    "    print(\"F1-Score: {:.2f}\".format(100 * metrics.f1_score(y, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_clf, X_test_b, y_test_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_clf, X_test_d, y_test_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_clf, X_test_m, y_test_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
